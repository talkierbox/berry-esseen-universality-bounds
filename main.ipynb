{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a596f72e-24a3-46ae-b1cf-8a88b7efbcd0",
   "metadata": {},
   "source": [
    "# Project 1 — Berry–Esseen Rate for Fixed Degree $d$\n",
    "\n",
    "Empirical confirmation that the overlap  \n",
    "$$X_N := \\sqrt{N}\\langle q, u_2\\rangle$$\n",
    "between a fixed test vector $q\\perp\\mathbf{1}$ and the second eigenvector $u_2$ of a **random $d$-regular graph** converges to $\\mathcal{N}(0, 1)$ at the **optimal Berry–Esseen rate**\n",
    "$$\\sup_x |\\mathbb{P}(X_N\\le x)-\\Phi(x)| = \\Theta(N^{-1/6}),$$\n",
    "as established in Nagel (2025) and Huang–Yau (2023).\n",
    "\n",
    "**Credits**: This notebook was written by [Hershraj Niranjani](https://hershrajn.com)\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "| Stage | What we do | Key parameters |\n",
    "|-------|------------|----------------|\n",
    "| **Graph generation** | Build $M \\approx 7,000$ simple $d$-regular graphs for every $(N, d)$ using edge-swapping Monte Carlo. | $d \\in \\{3,5,8\\}$; $N \\in \\{1,000, 10,000, 100,000, 500,000, 1,000,000\\}$; Rust backend with $3 \\times N \\times d$ edge swaps per graph. |\n",
    "| **Storage format** | Save as compressed sparse CSR matrices (`.npz` files) for memory efficiency. | `scipy.sparse.save_npz` with compression; adjacency matrices stored as uint8. |\n",
    "| **Spectral step** | Form the normalised adjacency $\\tilde{A} = A / \\sqrt{d-1}$ and extract $u_2$. | `scipy.sparse.linalg.eigsh` with `k=2`, `which=\"LA\"`, `tol=1e-2` |\n",
    "| **Statistic** | Compute $X_N$ with $q = e_1 - \\frac{1}{N}\\mathbf{1}$ (normalised). | Any deterministic $q \\perp \\mathbf{1}$ works. |\n",
    "| **Distance metric** | Kolmogorov–Smirnov distance $D_N = \\| F_N(x) - \\Phi(x)\\|$. | Use SciPy's `stats.kstest` with `norm.cdf` reference. |\n",
    "| **Rate extraction** | Linear regression of $\\log D_N$ on $\\log N$. | Slope $\\approx -1/6$ corroborates theory. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20587f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b825d-13a9-4dd6-9eab-5028f471e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This was ran in a python virtual environment with python version 3.13.5\n",
    "# Install the necessary dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f519ce-e7ce-47e2-94c8-5424148f1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import scipy.sparse.linalg as spla\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import kstest, norm, gaussian_kde\n",
    "import pandas as pd\n",
    "from typing import List, Sequence\n",
    "\n",
    "RANDOM_SEED = 42 # 42 for reproducibility\n",
    "\n",
    "BASE = Path(os.path.abspath(''))\n",
    "\n",
    "FIGURES = BASE / \"figures\"\n",
    "FIGURES.mkdir(exist_ok=True) # Ensure that the figures directory exists\n",
    "\n",
    "DATA = BASE / \"data\"\n",
    "DATA.mkdir(exist_ok=True) # Ensure that the data directory exists\n",
    "\n",
    "# First, generate graphs if they don't exist\n",
    "if not any(DATA.glob(\"d*/g_*.npz\")):\n",
    "    print(\"No graph files found. Running graph generation...\")\n",
    "    exec(open(\"generate_graphs.py\").read())\n",
    "    print(\"Graph generation complete.\")\n",
    "else:\n",
    "    print(\"Found existing graph files.\")\n",
    "\n",
    "NUM_GRAPHS_TO_LOAD = 2_000 # Number of graphs to load in per (n, d) configuration\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(file_names: Sequence[str], in_dir: str | Path) -> List[sp.csr_matrix]:\n",
    "    \"\"\"Load .npz CSR matrices (saved by generate_graphs.py) into sparse matrices.\"\"\"\n",
    "    graphs: List[sp.csr_matrix] = []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(in_dir) / file_name\n",
    "        A = sp.load_npz(file_path)\n",
    "        graphs.append(A)\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb0f47",
   "metadata": {},
   "source": [
    "## Run the analysis\n",
    "Make sure that you have generated the graphs using the `generate_graphs.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE         = 1024\n",
    "N_JOBS             = max(cpu_count() - 1, 1)\n",
    "\n",
    "def _x_stat_from_file(path: Path, q: np.ndarray) -> float:\n",
    "    try:\n",
    "        # Load CSR matrix directly (new format from generate_graphs.py)\n",
    "        A: sp.csr_matrix = sp.load_npz(path)\n",
    "        n = A.shape[0]\n",
    "        \n",
    "        # Verify this is actually a valid adjacency matrix\n",
    "        if A.shape[0] != A.shape[1]:\n",
    "            tqdm.write(f\"Error: {path} is not square matrix\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error loading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Convert to float32 for computation\n",
    "        A = A.astype(np.float32, copy=False)\n",
    "\n",
    "        # Get degree from first row (regular graph) - count nonzero entries\n",
    "        d = A.getrow(0).nnz\n",
    "        if d == 0:\n",
    "            tqdm.write(f\"Error: {path} has zero degree\")\n",
    "            return None\n",
    "            \n",
    "        # Normalize by sqrt(d-1) for spectral analysis\n",
    "        A.data *= 1.0 / np.sqrt(np.float32(max(1, d - 1)))\n",
    "\n",
    "        # Cast to float64 for eigsh\n",
    "        A_float64 = A.astype(np.float64, copy=False)\n",
    "        vals, vecs = spla.eigsh(A_float64, k=2, which=\"LA\", tol=1e-2, maxiter=1000)\n",
    "        \n",
    "        # Validate eigenvalues and eigenvectors\n",
    "        if not np.isfinite(vals).all():\n",
    "            tqdm.write(f\"Error: {path} has non-finite eigenvalues\")\n",
    "            return None\n",
    "            \n",
    "        if not np.isfinite(vecs).all():\n",
    "            tqdm.write(f\"Error: {path} has non-finite eigenvectors\")\n",
    "            return None\n",
    "        \n",
    "        # Get second largest eigenvalue's eigenvector\n",
    "        u2 = vecs[:, 0] if vals[0] < vals[1] else vecs[:, 1]\n",
    "        \n",
    "        # Ensure u2 is properly normalized and finite\n",
    "        if not np.isfinite(u2).all():\n",
    "            tqdm.write(f\"Error: {path} has non-finite u2\")\n",
    "            return None\n",
    "            \n",
    "        u2_norm = np.linalg.norm(u2)\n",
    "        if u2_norm == 0 or not np.isfinite(u2_norm):\n",
    "            tqdm.write(f\"Error: {path} has zero or invalid u2 norm\")\n",
    "            return None\n",
    "            \n",
    "        u2 = u2 / u2_norm  # Ensure unit normalization\n",
    "        \n",
    "        # Compute the overlap with bounds checking\n",
    "        overlap = np.dot(q, u2)\n",
    "        if not np.isfinite(overlap):\n",
    "            tqdm.write(f\"Error: {path} has non-finite overlap\")\n",
    "            return None\n",
    "            \n",
    "        result = np.sqrt(n) * float(overlap)\n",
    "        \n",
    "        # Final bounds check\n",
    "        if not np.isfinite(result):\n",
    "            tqdm.write(f\"Error: {path} has non-finite result\")\n",
    "            return None\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error computing eigenvector for {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Scan for available graph files with corrected pattern matching\n",
    "graph_files = []\n",
    "for d_folder in DATA.glob(\"d*\"):\n",
    "    if not d_folder.is_dir():\n",
    "        continue\n",
    "    graph_files.extend(d_folder.glob(\"g_*.npz\"))\n",
    "\n",
    "if not graph_files:\n",
    "    print(\"No graph files found! Please run generate_graphs.py first.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(graph_files)} graph files\")\n",
    "\n",
    "# Parse filenames: g_d{d}_n{n}_{seed}.npz\n",
    "pat = re.compile(r\"g_d(\\d+)_n(\\d+)_(\\d+)\\.npz\")\n",
    "groups: dict[tuple[int, int], list[Path]] = {}\n",
    "\n",
    "for p in graph_files:\n",
    "    match = pat.match(p.name)\n",
    "    if not match:\n",
    "        tqdm.write(f\"Warning: Skipping file with unexpected name format: {p.name}\")\n",
    "        continue\n",
    "    \n",
    "    d, n, seed = map(int, match.groups())\n",
    "    groups.setdefault((n, d), []).append(p)\n",
    "\n",
    "print(f\"Found graphs for {len(groups)} (n,d) combinations\")\n",
    "\n",
    "rows = []\n",
    "for (n, d), paths in sorted(groups.items()):\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "        \n",
    "    paths = paths[:NUM_GRAPHS_TO_LOAD]              # deterministic slice\n",
    "    q     = np.append([1.0], np.zeros(n - 1)) - 1.0 / n\n",
    "    q    /= np.linalg.norm(q)\n",
    "\n",
    "    X_vals = []\n",
    "\n",
    "    if n > 50_000:\n",
    "        current_batch_size = 128 # reduce memory usage for large n\n",
    "    else:\n",
    "        current_batch_size = BATCH_SIZE\n",
    "\n",
    "    for i in range(0, len(paths), current_batch_size):\n",
    "        batch = paths[i : i + current_batch_size]\n",
    "        X_batch = Parallel(\n",
    "            n_jobs=N_JOBS,\n",
    "            backend=\"loky\",\n",
    "            pre_dispatch=\"2*n_jobs\"                 # keeps memory bounded\n",
    "        )(delayed(_x_stat_from_file)(p, q) for p in batch)\n",
    "        \n",
    "        # Filter out None results\n",
    "        X_vals.extend([x for x in X_batch if x is not None])\n",
    "\n",
    "        tqdm.write(f\"[{n=}, {d=}] processed {i + len(batch):,}/{len(paths):,}\")\n",
    "    \n",
    "    if len(X_vals) < 10:  # Need minimum number of samples\n",
    "        tqdm.write(f\"Warning: Only got {len(X_vals)} valid samples for n={n}, d={d}\")\n",
    "        continue\n",
    "        \n",
    "    D_n, _ = kstest(X_vals, norm.cdf)\n",
    "    var_n    = float(np.var(X_vals, ddof=1))            # unbiased sample variance\n",
    "    hist_y, hist_x = np.histogram(\n",
    "        X_vals, bins=\"auto\", density=True\n",
    "    ) \n",
    "    kde_x = np.linspace(min(X_vals), max(X_vals), 200)\n",
    "    kde_y = gaussian_kde(X_vals)(kde_x)\n",
    "    rows.append(\n",
    "        dict(\n",
    "            n=n,\n",
    "            d=d,\n",
    "            D_n=D_n,\n",
    "            var_n=var_n,\n",
    "            n_graphs=len(X_vals),\n",
    "            hist_x=hist_x,           # bin edges  (len = k+1)\n",
    "            hist_y=hist_y,           # densities  (len = k)\n",
    "            kde_x=kde_x,           # uncomment if you keep KDE\n",
    "            kde_y=kde_y,\n",
    "        )\n",
    "    )\n",
    "    tqdm.write(\n",
    "        f\"[n={n:,}, d={d}] D_n={D_n:.4f}, Var={var_n:.3f} from {len(X_vals):,} graphs\"\n",
    "    )\n",
    "\n",
    "print(\"All batches done.\")\n",
    "full_df = pd.DataFrame(rows)\n",
    "if len(full_df) > 0:\n",
    "    print(f\"Successfully processed {len(full_df)} (n,d) configurations\")\n",
    "    display(full_df.head(50))\n",
    "else:\n",
    "    print(\"No data processed successfully. Check graph files and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db942f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(full_df) == 0:\n",
    "    print(\"No data to analyze. Please check graph generation and file paths.\")\n",
    "else:\n",
    "    limited_df = full_df[ full_df[\"d\"] < full_df[\"n\"] ** 0.25 ] # Ensure d(N) < N^0.25\n",
    "    res_df = limited_df.copy()\n",
    "    print(f\"Filtered to {len(res_df)} configurations satisfying d < N^0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize the Berry-Esseen rate for fixed d (KS distance)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for d, sub in res_df.groupby(\"d\"):\n",
    "    sub = sub.sort_values(\"n\")\n",
    "\n",
    "    xs = np.log10(sub[\"n\"].values)\n",
    "    ys = np.log10(sub[\"D_n\"].clip(lower=1e-12).values)  # avoid log10(0)\n",
    "\n",
    "    # least‑squares slope on log‑log scale\n",
    "    m, b = np.polyfit(xs, ys, 1)\n",
    "    ax.plot(xs, ys, \"o-\", label=fr\"$d={d}$  slope={m:.3f}\")\n",
    "\n",
    "# reference line with slope −1/6\n",
    "N_min, N_max = res_df[\"n\"].min(), res_df[\"n\"].max()\n",
    "ref_x = np.array([N_min, N_max])\n",
    "y0 = np.log10(res_df[\"D_n\"].max())          # anchor at left edge\n",
    "ref_y = y0 - (1/6) * (np.log10(ref_x) - np.log10(N_min))\n",
    "ax.plot(np.log10(ref_x), ref_y, \"--\", color=\"gray\", lw=1.2,\n",
    "        label=r\"reference slope $-1/6$\")\n",
    "\n",
    "ax.set_xlabel(r\"$\\log_{10} N$\")\n",
    "ax.set_ylabel(r\"$\\log_{10} D_N$\")\n",
    "ax.set_title(\"Berry-Esseen rate for fixed $d$ (KS distance)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a69d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Variance\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for d, sub in res_df.groupby(\"d\"):\n",
    "    sub = sub.sort_values(\"n\")\n",
    "    ax.plot(\n",
    "        np.log10(sub[\"n\"]),\n",
    "        sub[\"var_n\"],\n",
    "        \"o-\",\n",
    "        label=f\"d={d}\",\n",
    "    )\n",
    "\n",
    "ax.axhline(1.0, ls=\"--\", color=\"gray\", lw=1.2, label=\"theory σ² = 1\")\n",
    "ax.set_xlabel(r\"$\\log_{10} N$\")\n",
    "ax.set_ylabel(r\"sample variance  $\\operatorname{Var}[X_N]$\")\n",
    "ax.set_title(\"Variance of $X_N$ for fixed $d$\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the empirical density of X_N vs Standard Normal\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "for d in sorted(res_df[\"d\"].unique()):\n",
    "    row = res_df[res_df[\"d\"] == d].sort_values(\"n\").iloc[-1]  # largest N for this d\n",
    "    ax.plot(\n",
    "        row[\"kde_x\"], row[\"kde_y\"],\n",
    "        label=f\"d={d}, N={row['n']:,}\"\n",
    "    )\n",
    "\n",
    "# standard normal pdf for reference\n",
    "x_grid = np.linspace(-4, 4, 400)\n",
    "ax.plot(x_grid, norm.pdf(x_grid), \"k--\", lw=1.5, label=\"N(0, 1)\")\n",
    "\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(\"density\")\n",
    "ax.set_title(\"Empirical density of $X_N$ vs Standard Normal\")\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac418e",
   "metadata": {},
   "source": [
    "# Project 2 — Berry–Esseen Rate in the Sparse-Growing-Degree Regime\n",
    "\n",
    "We now let the degree grow slowly with graph size,  \n",
    "$$d = d(N) \\le N^{1/4},$$\n",
    "and test the refined prediction (Nagel 2025; Huang–Yau 2023) that the **rescaled KS error**\n",
    "\n",
    "$$\\tilde{D}_N = \\frac{D_N}{\\sqrt{d}}$$\n",
    "\n",
    "still obeys the optimal rate  \n",
    "\n",
    "$$\\tilde{D}_N = \\Theta(N^{-1/6}).$$\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "$$\\sup_x \\left|\\mathbb{P}(X_N \\le x) - \\Phi(x)\\right| = \\Theta(\\sqrt{d}N^{-1/6}),$$\n",
    "\n",
    "where $X_N = \\sqrt{N}\\langle q, u_2\\rangle$.\n",
    "\n",
    "---\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "| Stage | What we do | Key parameters |\n",
    "|-------|------------|----------------|\n",
    "| **Graph generation** | Build simple $d(N)$-regular graphs using the same edge-swapping Monte Carlo method as Project 1. | $d \\in \\{3,5,8,10\\}$ across $N \\in \\{1,000, 10,000, 50,000\\}$; analyze $(N,d)$ pairs satisfying $d \\leq N^{0.25}$; $\\approx$ 5k graphs per configuration. |\n",
    "| **Backend optimization** | Leverage Rust backend for fast batch generation with adaptive batch sizes based on $N$. | Batch size: 512 for $N \\leq 10k$, 256 for $N \\leq 50k$, 64 for $N > 50k$ to manage memory. |\n",
    "| **Storage format** | Same compressed sparse CSR format as Project 1 for consistent memory usage. | `scipy.sparse.save_npz` with uint8 adjacency data and int32/int64 indices based on $N$. |\n",
    "| **Spectral step** | Form the normalised adjacency $\\tilde{A} = A / \\sqrt{d-1}$ and extract $u_2$. | Same `scipy.sparse.linalg.eigsh` configuration as Project 1. |\n",
    "| **Statistic** | $X_N = \\sqrt{N}\\langle q, u_2\\rangle$ with $q = e_1 - \\frac{1}{N}\\mathbf{1}$ (normalised). | Same deterministic $q \\perp \\mathbf{1}$ as in Project 1. |\n",
    "| **Distance metric** | $\\tilde{D}_N = D_N / \\sqrt{d}$ where $D_N$ is the KS distance between $X_N$ and $\\mathcal{N}(0,1)$. | `scipy.stats.kstest` with rescaling by $\\sqrt{d}$ |\n",
    "| **Rate extraction** | Regress $\\log_{10}\\tilde{D}_N$ on $\\log_{10}N$. | Slope $\\approx -1/6$ confirms universality after the $\\sqrt{d}$ factor. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scaled KS distance for growing degree d(N) ≤ N^0.25\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# points: all (n,d) pairs, colour‑by‑d for readability\n",
    "scatter = ax.scatter(\n",
    "    np.log10(limited_df[\"n\"]),\n",
    "    np.log10(limited_df[\"D_n\"] / np.sqrt(limited_df[\"d\"])),\n",
    "    c=limited_df[\"d\"], cmap=\"viridis\", s=35\n",
    ")\n",
    "\n",
    "# global least‑squares slope in log–log space\n",
    "xs = np.log10(limited_df[\"n\"].values)\n",
    "ys = np.log10(limited_df[\"D_n\"].values / np.sqrt(limited_df[\"d\"].values))\n",
    "m, b = np.polyfit(xs, ys, 1)\n",
    "ax.plot(xs, m * xs + b, \"k--\", lw=1.5, label=f\"fit slope = {m:.3f}\")\n",
    "\n",
    "# reference – theoretical slope −1/6\n",
    "N_min, N_max = limited_df[\"n\"].min(), limited_df[\"n\"].max()\n",
    "ref_x = np.array([N_min, N_max])\n",
    "ref_y = b - (1 / 6) * (np.log10(ref_x) - np.log10(N_min))\n",
    "ax.plot(np.log10(ref_x), ref_y, \"r:\", lw=1.2, label=f\"expected slope {(-1 / 6):.3f}\")\n",
    "\n",
    "ax.set_xlabel(\"log10 N\")\n",
    "ax.set_ylabel(\"log10 [D_N / sqrt(d)]\")\n",
    "ax.set_title(\"Scaled KS distance for growing degree d(N) ≤ N^0.25\")\n",
    "ax.legend()\n",
    "cbar = plt.colorbar(scatter, ax=ax, label=\"degree d\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this looks for points not covered by the bound d(N) ≤ N^0.25\n",
    "# ── Points with d(N) > N^0.25  ──────────────────────────────────────────────\n",
    "outside_df = full_df[full_df[\"d\"] > full_df[\"n\"] ** 0.25]\n",
    "\n",
    "if len(outside_df) == 0:\n",
    "    print(\"No points outside the bound found.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # scatter: log‑log of scaled KS distance, colour by degree\n",
    "    scatter = ax.scatter(\n",
    "        np.log10(outside_df[\"n\"]),\n",
    "        np.log10(outside_df[\"D_n\"] / np.sqrt(outside_df[\"d\"])),\n",
    "        c=outside_df[\"d\"],\n",
    "        cmap=\"plasma\",\n",
    "        s=35,\n",
    "        marker=\"o\",\n",
    "        label=\"outside bound\"\n",
    "    )\n",
    "\n",
    "    # least‑squares slope for these out‑of‑bound points\n",
    "    xs = np.log10(outside_df[\"n\"].values)\n",
    "    ys = np.log10(outside_df[\"D_n\"].values / np.sqrt(outside_df[\"d\"].values))\n",
    "    m, b = np.polyfit(xs, ys, 1)\n",
    "    ax.plot(xs, m * xs + b, \"k--\", lw=1.5, label=f\"fit slope = {m:.3f}\")\n",
    "\n",
    "    # theoretical reference: slope −1/6 anchored at smallest N\n",
    "    N_min, N_max = outƒside_df[\"n\"].min(), outside_df[\"n\"].max()\n",
    "    ref_x = np.array([N_min, N_max])\n",
    "    ref_y = b - (1 / 6) * (np.log10(ref_x) - np.log10(N_min))\n",
    "    ax.plot(np.log10(ref_x), ref_y, \"r:\", lw=1.2, label=\"expected slope -1/6\")\n",
    "\n",
    "    ax.set_xlabel(\"log10 N\")\n",
    "    ax.set_ylabel(\"log10 [D_N / sqrt(d)]\")\n",
    "    ax.set_title(\"Scaled KS distance for d(N) > N^0.25 (outside bound)\")\n",
    "    ax.legend()\n",
    "    plt.colorbar(scatter, ax=ax, label=\"degree d\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
